{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khlebovich-Alexandra/horoscope_generator/blob/master/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6ae7362-3e08-456b-fc0b-d456366b16fd"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1KtZ8OTz5Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts = pd.read_csv('https://raw.githubusercontent.com/Khlebovich-Alexandra/horoscope_generator/master/Data/final_posts.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mV45ELxx_X5",
        "colab_type": "code",
        "outputId": "1a7f68c9-81bc-4b53-a640-8a73f17c2519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "posts"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "      <th>date</th>\n",
              "      <th>type</th>\n",
              "      <th>index in posts</th>\n",
              "      <th>domain</th>\n",
              "      <th>index_before_concat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>сегодня вы можете почувствовать незащищенность...</td>\n",
              "      <td>339</td>\n",
              "      <td>11 декабря</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "      <td>ribyhoroscop</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>вам звёзды рекомендуют сегодня больше работать...</td>\n",
              "      <td>303</td>\n",
              "      <td>11 декабря</td>\n",
              "      <td>love</td>\n",
              "      <td>0</td>\n",
              "      <td>ribyhoroscop</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>если сегодня вы почувствуете робость, или, тог...</td>\n",
              "      <td>215</td>\n",
              "      <td>11 декабря</td>\n",
              "      <td>simple</td>\n",
              "      <td>0</td>\n",
              "      <td>ribyhoroscop</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>сегодня звезды рекомендуют вам заняться коррек...</td>\n",
              "      <td>278</td>\n",
              "      <td>10 декабря</td>\n",
              "      <td>business</td>\n",
              "      <td>1</td>\n",
              "      <td>ribyhoroscop</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>сегодня ваш любимый человек признается в том, ...</td>\n",
              "      <td>273</td>\n",
              "      <td>10 декабря</td>\n",
              "      <td>love</td>\n",
              "      <td>1</td>\n",
              "      <td>ribyhoroscop</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26957</th>\n",
              "      <td>день преобразования космических энергий, получ...</td>\n",
              "      <td>544</td>\n",
              "      <td>2 марта</td>\n",
              "      <td>simple</td>\n",
              "      <td>2648</td>\n",
              "      <td>strelechoroscop</td>\n",
              "      <td>4575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26958</th>\n",
              "      <td>завтра - полон перемен для вас. поэтому строит...</td>\n",
              "      <td>256</td>\n",
              "      <td>1 марта</td>\n",
              "      <td>simple</td>\n",
              "      <td>2649</td>\n",
              "      <td>strelechoroscop</td>\n",
              "      <td>4576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26959</th>\n",
              "      <td>утром не мешкая приступайте к работе или завяз...</td>\n",
              "      <td>441</td>\n",
              "      <td>28 февраля</td>\n",
              "      <td>simple</td>\n",
              "      <td>2650</td>\n",
              "      <td>strelechoroscop</td>\n",
              "      <td>4577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26960</th>\n",
              "      <td>общительность и дружелюбие вас сегодня могут н...</td>\n",
              "      <td>453</td>\n",
              "      <td>27 февраля</td>\n",
              "      <td>simple</td>\n",
              "      <td>2651</td>\n",
              "      <td>strelechoroscop</td>\n",
              "      <td>4578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26961</th>\n",
              "      <td>свободное общение может стать затруднительным....</td>\n",
              "      <td>505</td>\n",
              "      <td>26 февраля</td>\n",
              "      <td>simple</td>\n",
              "      <td>2652</td>\n",
              "      <td>strelechoroscop</td>\n",
              "      <td>4579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26962 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...  index_before_concat\n",
              "0      сегодня вы можете почувствовать незащищенность...  ...                    0\n",
              "1      вам звёзды рекомендуют сегодня больше работать...  ...                    1\n",
              "2      если сегодня вы почувствуете робость, или, тог...  ...                    2\n",
              "3      сегодня звезды рекомендуют вам заняться коррек...  ...                    3\n",
              "4      сегодня ваш любимый человек признается в том, ...  ...                    4\n",
              "...                                                  ...  ...                  ...\n",
              "26957  день преобразования космических энергий, получ...  ...                 4575\n",
              "26958  завтра - полон перемен для вас. поэтому строит...  ...                 4576\n",
              "26959  утром не мешкая приступайте к работе или завяз...  ...                 4577\n",
              "26960  общительность и дружелюбие вас сегодня могут н...  ...                 4578\n",
              "26961  свободное общение может стать затруднительным....  ...                 4579\n",
              "\n",
              "[26962 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FFkwZek_59l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts.drop(['date', 'type', 'index in posts', 'domain', 'index_before_concat'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82TFR9vpzuo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts.drop(np.argmax(posts.length), inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7aWB6Y6psw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_spaces(x):\n",
        "    literals = ',.?!;:()'\n",
        "    for literal in literals:\n",
        "        if literal in x:\n",
        "            x = x.replace(literal, ' ' + literal + ' ')\n",
        "            \n",
        "    pattern_bad_spaces_1 = re.compile(r'(^ +)|( +$)')\n",
        "    pattern_bad_spaces_2 = re.compile(r'(  +)|(\\t)')\n",
        "    x = re.sub(pattern_bad_spaces_1, '', x)\n",
        "    x = re.sub(pattern_bad_spaces_2, ' ', x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSO91hZP2IqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split(x):\n",
        "    res = re.split(re.compile(r'[ ]+'), x.lower())\n",
        "    while '' in res:\n",
        "        res.remove('')\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPVgqjg4kAEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vocab(texts):\n",
        "    texts = pd.Series(texts)\n",
        "    texts = texts.map(add_spaces)\n",
        "    texts = texts.map(split)\n",
        "    word_index = {}\n",
        "    word_index[\"<Заполнитель>\"] = 0\n",
        "    word_index[\"<Начало последовательности>\"] = 1\n",
        "    word_index[\"<Не используется>\"] = 2\n",
        "    word_index[\"<Неизвестное слово>\"] = 3\n",
        "    ind = 4\n",
        "    for text in texts:\n",
        "        for word in text:\n",
        "            if not (word in word_index):\n",
        "                word_index[word] = ind\n",
        "                ind += 1\n",
        "    return word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcWQdfGC9faV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_to_int(word):\n",
        "    if word in word_index:\n",
        "        return word_index[word]\n",
        "    return 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgw9b2NvOZsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_by_index(index):\n",
        "    for word, value in word_index.items():\n",
        "        if index == value:\n",
        "            return word\n",
        "    return ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M98ZykAWi58p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(text):\n",
        "    text = add_spaces(text)\n",
        "    text = split(text)\n",
        "    text = list(map(word_to_int, text))\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20IzUjIekiq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = create_vocab(posts.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiWbROst5VT0",
        "colab_type": "code",
        "outputId": "999f7b73-8077-417f-8c5a-9f0314232d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_index['.']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaHhwmDX5VdJ",
        "colab_type": "code",
        "outputId": "783ee569-754a-4e89-a173-5f2d3fae4609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD_ovvDAlZdC",
        "colab_type": "code",
        "outputId": "e7b026b0-73a6-4b32-bbad-7fa8d231a57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(create_dataset(posts.text[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 9, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 14, 28, 29, 30, 31, 32, 33, 19, 34, 35, 36, 37, 19, 38, 39, 40, 41, 42, 43, 44, 14, 45, 46, 47, 25, 48, 49, 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_um9i5Z5Vlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts.text = posts.text.map(create_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODFF1IxCYLnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts.length = posts.text.map(len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnx8JPUW_iBH",
        "colab_type": "text"
      },
      "source": [
        "Save DataFrame and vocabluary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu2ypVPl_frW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts.to_csv('posts_word_to_int.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFtZFyOh_fv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('vocab.json', 'w') as fp:\n",
        "    json.dump(word_index, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1aY0m00BuLP",
        "colab_type": "text"
      },
      "source": [
        "Padding seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXpZnNsJbo07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_list_of_ints(x):\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip('][').split(', ')\n",
        "    return list(map(int, x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbv3YeyiW_29",
        "colab_type": "code",
        "outputId": "c20e3512-c9cf-45aa-e3d3-c0770a8d8a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "posts = pd.read_csv('posts_word_to_int.csv', index_col=0)\n",
        "posts.text = posts.text.map(get_list_of_ints)\n",
        "posts"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[50, 51, 52, 4, 53, 54, 14, 55, 56, 57, 58, 59...</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[89, 4, 5, 90, 91, 19, 56, 19, 92, 93, 94, 19,...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[4, 109, 52, 50, 110, 111, 56, 112, 113, 114, ...</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[4, 136, 137, 138, 139, 11, 140, 19, 20, 141, ...</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26957</th>\n",
              "      <td>[199, 7429, 7906, 19954, 19, 8101, 20213, 8982...</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26958</th>\n",
              "      <td>[3016, 81, 2294, 3598, 47, 25, 14, 302, 6957, ...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26959</th>\n",
              "      <td>[8803, 121, 47918, 13480, 71, 535, 56, 35595, ...</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26960</th>\n",
              "      <td>[8673, 9, 5898, 25, 4, 702, 23496, 87, 19815, ...</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26961</th>\n",
              "      <td>[4187, 2385, 254, 595, 7041, 14, 5, 6, 160, 36...</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26961 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  length\n",
              "0      [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...      53\n",
              "1      [50, 51, 52, 4, 53, 54, 14, 55, 56, 57, 58, 59...      49\n",
              "2      [89, 4, 5, 90, 91, 19, 56, 19, 92, 93, 94, 19,...      40\n",
              "3      [4, 109, 52, 50, 110, 111, 56, 112, 113, 114, ...      46\n",
              "4      [4, 136, 137, 138, 139, 11, 140, 19, 20, 141, ...      46\n",
              "...                                                  ...     ...\n",
              "26957  [199, 7429, 7906, 19954, 19, 8101, 20213, 8982...      81\n",
              "26958  [3016, 81, 2294, 3598, 47, 25, 14, 302, 6957, ...      40\n",
              "26959  [8803, 121, 47918, 13480, 71, 535, 56, 35595, ...      63\n",
              "26960  [8673, 9, 5898, 25, 4, 702, 23496, 87, 19815, ...      68\n",
              "26961  [4187, 2385, 254, 595, 7041, 14, 5, 6, 160, 36...      88\n",
              "\n",
              "[26961 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQTvyMT9zoc6",
        "colab_type": "code",
        "outputId": "1d25bc86-12af-41d0-8e8d-a553efd2a2b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "posts.describe()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>26961.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>50.237640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>15.919764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>48.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>58.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>129.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             length\n",
              "count  26961.000000\n",
              "mean      50.237640\n",
              "std       15.919764\n",
              "min        4.000000\n",
              "25%       40.000000\n",
              "50%       48.000000\n",
              "75%       58.000000\n",
              "max      129.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67kAP_hvz5Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AargE3YdaTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = pad_sequences(posts.text, maxlen, padding='post')\n",
        "texts = tf.data.Dataset.from_tensor_slices(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKOQB0YMg_u0",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = texts.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SayoM0ZdhDYu",
        "colab_type": "code",
        "outputId": "7724ad99-ce0b-4c66-99f1-807283551e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((49,), (49,)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXU2AHXkiaoX",
        "colab_type": "code",
        "outputId": "97f4dd37-9fc8-48c8-fff3-baff0a884e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 49), (64, 49)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxYcQI5uCs7u",
        "colab_type": "code",
        "outputId": "7ad412aa-5847-4583-cb89-696da14b9ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# define model\n",
        "vocab_size = len(word_index)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[BATCH_SIZE, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           12268288  \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 47923)         49121075  \n",
            "=================================================================\n",
            "Total params: 65,327,667\n",
            "Trainable params: 65,327,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6530998e-584a-4db1-f576-a209605b205a",
        "id": "RzjfulxIp3hZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 49, 47923) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqFMUQc_UFgM",
        "outputId": "9224e789-9216-4b72-fc92-e762d36d6f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43300, 36774,  3959, 19223, 12408, 17769, 35770, 31458, 18555,\n",
              "       16962, 37015, 43176, 20071,  6699, 10066, 35625,   267, 21318,\n",
              "       26535, 39679, 29277, 19953, 11723, 38912, 38630, 36330,  3403,\n",
              "       44143, 23315, 15201, 14070, 34031, 32983, 43160, 43281, 21253,\n",
              "        6120, 35009, 17436, 31376, 27662, 27922, 44843, 22289,  4526,\n",
              "       13606, 29302, 40242, 26342])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "outputId": "14d6f268-7814-47f2-9af1-11b157093f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\" \".join([get_word_by_index(i) for i in input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\" \".join([get_word_by_index(i) for i in sampled_indices])))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'вас , кажется , начинается полоса неудач . и с начальником отношения не ладятся , и с новым проектом возникли трудности , которые что-то никак не разрешаются . впрочем , невозможно всё время быть на коне , лучше подойдите к этим проблемам философски и найдите в ситуации положительные стороны'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'доверчиво дребезгом первая опирайтесь недочеты предоставите служения надоедают приготовленное выходящее обжитое бесплатные вестись невзначай неплохого деликатесы главное магических стабильна определённым потратил пробуждения официально мыслят вымещать парить подзаработать напоминающей существу оживление поспать пошатнется исчезло намагничены запретные перессоритесь конкретного песочные поминайте погружены соскучиться собеседникам доходными неверная непрофессиональных помощниками подколам нежданные узкой'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "outputId": "48fd682b-80df-44d7-b645-d4e06a3817e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 49, 47923)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       10.777466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDl1_Een6rL0",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7yGBE2zxMMHs",
        "colab": {}
      },
      "source": [
        "EPOCHS=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "outputId": "0c979daf-cd9a-431a-9723-3223730c9caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 421 steps\n",
            "Epoch 1/100\n",
            "421/421 [==============================] - 91s 217ms/step - loss: 6.5601\n",
            "Epoch 2/100\n",
            "421/421 [==============================] - 90s 214ms/step - loss: 5.8502\n",
            "Epoch 3/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 5.4948\n",
            "Epoch 4/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 5.2361\n",
            "Epoch 5/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 4.9957\n",
            "Epoch 6/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 4.5184\n",
            "Epoch 7/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 3.9648\n",
            "Epoch 8/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 3.3192\n",
            "Epoch 9/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 2.7647\n",
            "Epoch 10/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 2.3445\n",
            "Epoch 11/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 2.0394\n",
            "Epoch 12/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 1.8097\n",
            "Epoch 13/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 1.6304\n",
            "Epoch 14/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 1.4876\n",
            "Epoch 15/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 1.3684\n",
            "Epoch 16/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 1.2699\n",
            "Epoch 17/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 1.1883\n",
            "Epoch 18/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 1.1151\n",
            "Epoch 19/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 1.0541\n",
            "Epoch 20/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.9987\n",
            "Epoch 21/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.9515\n",
            "Epoch 22/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.9087\n",
            "Epoch 23/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.8712\n",
            "Epoch 24/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.8355\n",
            "Epoch 25/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.8055\n",
            "Epoch 26/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.7773\n",
            "Epoch 27/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.7524\n",
            "Epoch 28/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.7286\n",
            "Epoch 29/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.7077\n",
            "Epoch 30/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.6875\n",
            "Epoch 31/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.6719\n",
            "Epoch 32/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.6527\n",
            "Epoch 33/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.6389\n",
            "Epoch 34/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.6211\n",
            "Epoch 35/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.6094\n",
            "Epoch 36/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5974\n",
            "Epoch 37/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 0.5896\n",
            "Epoch 38/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5790\n",
            "Epoch 39/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5687\n",
            "Epoch 40/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5576\n",
            "Epoch 41/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5483\n",
            "Epoch 42/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5455\n",
            "Epoch 43/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5368\n",
            "Epoch 44/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5284\n",
            "Epoch 45/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5216\n",
            "Epoch 46/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5156\n",
            "Epoch 47/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.5148\n",
            "Epoch 48/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5066\n",
            "Epoch 49/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5015\n",
            "Epoch 50/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.5000\n",
            "Epoch 51/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4954\n",
            "Epoch 52/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.4886\n",
            "Epoch 53/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 0.4868\n",
            "Epoch 54/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 0.4828\n",
            "Epoch 55/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 0.4798\n",
            "Epoch 56/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4770\n",
            "Epoch 57/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4767\n",
            "Epoch 58/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 0.4734\n",
            "Epoch 59/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4719\n",
            "Epoch 60/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4701\n",
            "Epoch 61/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4661\n",
            "Epoch 62/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4680\n",
            "Epoch 63/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4676\n",
            "Epoch 64/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4631\n",
            "Epoch 65/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4626\n",
            "Epoch 66/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4609\n",
            "Epoch 67/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4591\n",
            "Epoch 68/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4562\n",
            "Epoch 69/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4545\n",
            "Epoch 70/100\n",
            "421/421 [==============================] - 89s 211ms/step - loss: 0.4559\n",
            "Epoch 71/100\n",
            "421/421 [==============================] - 89s 210ms/step - loss: 0.4545\n",
            "Epoch 72/100\n",
            "421/421 [==============================] - 89s 210ms/step - loss: 0.4517\n",
            "Epoch 73/100\n",
            "421/421 [==============================] - 89s 210ms/step - loss: 0.4533\n",
            "Epoch 74/100\n",
            "421/421 [==============================] - 88s 210ms/step - loss: 0.4553\n",
            "Epoch 75/100\n",
            "421/421 [==============================] - 88s 210ms/step - loss: 0.4566\n",
            "Epoch 76/100\n",
            "421/421 [==============================] - 88s 210ms/step - loss: 0.4579\n",
            "Epoch 77/100\n",
            "421/421 [==============================] - 88s 209ms/step - loss: 0.4572\n",
            "Epoch 78/100\n",
            "421/421 [==============================] - 88s 210ms/step - loss: 0.4535\n",
            "Epoch 79/100\n",
            "421/421 [==============================] - 89s 210ms/step - loss: 0.4518\n",
            "Epoch 80/100\n",
            "421/421 [==============================] - 89s 211ms/step - loss: 0.4538\n",
            "Epoch 81/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4564\n",
            "Epoch 82/100\n",
            "421/421 [==============================] - 89s 213ms/step - loss: 0.4539\n",
            "Epoch 83/100\n",
            "421/421 [==============================] - 90s 213ms/step - loss: 0.4527\n",
            "Epoch 84/100\n",
            "421/421 [==============================] - 90s 214ms/step - loss: 0.4547\n",
            "Epoch 85/100\n",
            "421/421 [==============================] - 90s 214ms/step - loss: 0.4545\n",
            "Epoch 86/100\n",
            "421/421 [==============================] - 90s 214ms/step - loss: 0.4536\n",
            "Epoch 87/100\n",
            "421/421 [==============================] - 90s 214ms/step - loss: 0.4544\n",
            "Epoch 88/100\n",
            "421/421 [==============================] - 90s 214ms/step - loss: 0.4583\n",
            "Epoch 89/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4592\n",
            "Epoch 90/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4610\n",
            "Epoch 91/100\n",
            "421/421 [==============================] - 89s 211ms/step - loss: 0.4629\n",
            "Epoch 92/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4651\n",
            "Epoch 93/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4649\n",
            "Epoch 94/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4662\n",
            "Epoch 95/100\n",
            "421/421 [==============================] - 89s 212ms/step - loss: 0.4675\n",
            "Epoch 96/100\n",
            "421/421 [==============================] - 89s 211ms/step - loss: 0.4686\n",
            "Epoch 97/100\n",
            "421/421 [==============================] - 89s 211ms/step - loss: 0.4699\n",
            "Epoch 98/100\n",
            "421/421 [==============================] - 89s 211ms/step - loss: 0.4769\n",
            "Epoch 99/100\n",
            "421/421 [==============================] - 88s 210ms/step - loss: 0.4783\n",
            "Epoch 100/100\n",
            "421/421 [==============================] - 89s 210ms/step - loss: 0.4827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA2vLSn5wM2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "json_file = open('posts_model_words.json', 'w')\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights('posts_model_words.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "model.load_weights('posts_model_words.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71xa6jnYVrAN",
        "outputId": "76d20844-c745-4757-c14d-ab8f0e1da91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            12268288  \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 47923)          49121075  \n",
            "=================================================================\n",
            "Total params: 65,327,667\n",
            "Trainable params: 65,327,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string, num_of_words, temp):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  text = start_string\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_of_words):\n",
        "      input_eval = create_dataset(start_string)\n",
        "      input_eval = pad_sequences([input_eval], maxlen - 1, padding='post')\n",
        "\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      new_word = get_word_by_index(predicted_id)\n",
        "      text = text + ' ' + new_word\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvUY75TeXDxF",
        "colab_type": "code",
        "outputId": "97773a42-984d-4222-9532-080c6e0d16fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_text(model, 'сегодня вам ', 10, 4.5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'сегодня вам  тона посидеть запутано изделия поладите работа <Заполнитель> <Заполнитель> осуществлением способностями'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVNddTOde3Pu",
        "colab_type": "code",
        "outputId": "64cf127a-24b6-435b-ec4b-c72c8d5cf55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_text(model, 'сегодня вам ', 10, 55)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'сегодня вам  выражаться сидело прозябать обещающий кормления удержитесь перспективах метеор выводя испортятся'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fRyJvyBe3b8",
        "colab_type": "code",
        "outputId": "1679c2ec-e999-4290-9196-f7f1c6abbc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_text(model, 'сегодня вам ', 10, 28)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'сегодня вам  возжелать незнакомцам пригорит собственноручно легких расспросы критичный переставьте близкое предлагаемый'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1IMj6iKw-ZH",
        "colab_type": "text"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f9234e35-9214-4587-c4a3-88bb2f3524ed",
        "id": "LDNMkBBWw2eP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "posts_model_words.h5\tposts_word_to_int.csv  vocab.json\n",
            "posts_model_words.json\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BshduFK_5xMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "json_file = open('posts_model.json', 'w')\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights('posts_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ZV2H0X6lye",
        "colab_type": "code",
        "outputId": "f49fe1c7-c355-4d37-d61a-52d11d0b16c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "posts_word_to_int.csv  sample_data  vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR0iEZPy6mcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XReJFqs_xM6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "804ec168-c49d-4f3d-a3a6-4df3c41cf2da"
      },
      "source": [
        "files.download('posts_model.json')\n",
        "files.download('posts_model.h5')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 42308, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YSD4cazExSka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c3105fc6-1826-4dad-84be-69c459ec0ab4"
      },
      "source": [
        "files.download('posts_model_words.json')\n",
        "files .download('posts_model_words.h5')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-805c44519f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'posts_model_words.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'posts_model_words.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Qi8Vqt7BNf",
        "colab_type": "text"
      },
      "source": [
        "# Generate by symbols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFNBXwalS6z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts = pd.read_csv('https://raw.githubusercontent.com/Khlebovich-Alexandra/horoscope_generator/master/Data/final_posts.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCFXJXt9yQHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a51d8c81-1f94-4707-8c9b-62a029f4a621"
      },
      "source": [
        "text = posts.text\n",
        "text = '\\n'.join(text)\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 8121558 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5gVHG7ZzTaX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4d58e168-e316-4150-c6b0-595134af9e0d"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня вы можете почувствовать незащищенность и неуверенность в собственных силах. попробуйте проанализировать ситуацию и выяснить, что же так сильно выбило вас из колеи. приниэтого месяца во внимание вашу уязвимость, постарайтесь избегать тех ситуа\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ0ohEQKzWJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07132438-f185-44c6-8474-eb54c305e413"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLKxnJOpzaO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMAQpucpzh-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "16320d37-fe45-47c7-be86-2a52e0a70b57"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "с\n",
            "е\n",
            "г\n",
            "о\n",
            "д\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsDKTcD1zpEj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2ec74531-b121-4a3f-bc04-797f516ec5ee"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'сегодня вы можете почувствовать незащищенность и неуверенность в собственных силах. попробуйте проана'\n",
            "'лизировать ситуацию и выяснить, что же так сильно выбило вас из колеи. приниэтого месяца во внимание '\n",
            "'вашу уязвимость, постарайтесь избегать тех ситуаций, которые требуют проявления всех стальных черт ха'\n",
            "'рактера. сейчас это для вас будет трудновато.\\nвам звёзды рекомендуют сегодня больше работать. ну или '\n",
            "'хотя бы демонстрировать окружающим имитацию деятельности столь бурной, что у них бы отпало всякое жел'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foppb26-zvQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC4t3kvOz6EO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "233201c9-bf5d-4c63-b15b-d312e30c5a89"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'сегодня вы можете почувствовать незащищенность и неуверенность в собственных силах. попробуйте проан'\n",
            "Target data: 'егодня вы можете почувствовать незащищенность и неуверенность в собственных силах. попробуйте проана'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMPENjlaz9tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0cd29af6-11ab-43cd-d857-53c590fb3e2e"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 67 ('с')\n",
            "  expected output: 55 ('е')\n",
            "Step    1\n",
            "  input: 55 ('е')\n",
            "  expected output: 53 ('г')\n",
            "Step    2\n",
            "  input: 53 ('г')\n",
            "  expected output: 64 ('о')\n",
            "Step    3\n",
            "  input: 64 ('о')\n",
            "  expected output: 54 ('д')\n",
            "Step    4\n",
            "  input: 54 ('д')\n",
            "  expected output: 63 ('н')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8kJPO5b0BVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72d6443b-115a-4f5b-abf2-a9ddaa849ae2"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99E0F3340TKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c8a91a4a-e089-449e-8abb-84f762fab383"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[BATCH_SIZE, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (64, None, 256)           22016     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (64, None, 86)            88150     \n",
            "=================================================================\n",
            "Total params: 4,048,470\n",
            "Trainable params: 4,048,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8uzxKRh0foI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f401cb12-da18-411c-e024-07ce28277aab"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 86) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgkReS80iux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ9WKMdo0m7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "486e5872-6504-44e7-ec73-9b87db237ad6"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([53, 15, 36, 74, 22, 14, 52, 78,  8, 41, 24, 67, 10, 36, 45,  1, 84,\n",
              "        7, 41,  3,  0, 68,  5, 64,  5, 35, 30, 36, 74, 74, 33, 78, 25, 31,\n",
              "        5, 69, 81, 67, 54, 14, 70, 44, 60, 80, 62, 68, 81, 29, 12, 11, 48,\n",
              "       74, 30, 50, 72, 69,  8, 33, 77,  4, 34, 75, 40, 19, 85, 38, 19, 31,\n",
              "       74, 63, 72, 76, 18, 61, 44, 79, 13,  2, 56, 74, 85, 10, 84, 85, 69,\n",
              "       41, 23, 24, 51, 68, 60, 17, 38, 71, 21,  7, 13, 20, 78, 36])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siJUC_P50qOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f4f1bd54-800e-49e7-f186-a44f8a0c6837"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'ить профессию и уехать работать за границу. завтра  вам нужно быть немного отчаяннее и рискованнее, '\n",
            "\n",
            "Next Char Predictions: \n",
            " 'г2mш91вь+r;с-mv “*r#\\nт(о(ldmшшiь=e(уясд1фuкюмтяc/.«шdацу+iы%kщq6”o6eшнцъ5лuэ0!жш”-“”уr:;бтк4oх8*07ьm'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXCluY-90u8J",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhq67shL0zjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-8uDzfU06Ii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6d50ae0-81cf-430d-fc08-8fad8efddb84"
      },
      "source": [
        "history1 = model.fit(dataset, epochs=80)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1256 steps\n",
            "Epoch 1/80\n",
            "  45/1256 [>.............................] - ETA: 2:09 - loss: 3.3538"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 37828, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1256/1256 [==============================] - 55s 44ms/step - loss: 1.5446\n",
            "Epoch 2/80\n",
            "1256/1256 [==============================] - 53s 43ms/step - loss: 1.0817\n",
            "Epoch 3/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0066\n",
            "Epoch 4/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9655\n",
            "Epoch 5/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9398\n",
            "Epoch 6/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9231\n",
            "Epoch 7/80\n",
            "1256/1256 [==============================] - 54s 43ms/step - loss: 0.9131\n",
            "Epoch 8/80\n",
            "1256/1256 [==============================] - 53s 43ms/step - loss: 0.9087\n",
            "Epoch 9/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9073\n",
            "Epoch 10/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9091\n",
            "Epoch 11/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9134\n",
            "Epoch 12/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9213\n",
            "Epoch 13/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9306\n",
            "Epoch 14/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9454\n",
            "Epoch 15/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9616\n",
            "Epoch 16/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 0.9910\n",
            "Epoch 17/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.0356\n",
            "Epoch 18/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.1766\n",
            "Epoch 19/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.7669\n",
            "Epoch 20/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.6216\n",
            "Epoch 21/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.4844\n",
            "Epoch 22/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.3734\n",
            "Epoch 23/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.2878\n",
            "Epoch 24/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.2249\n",
            "Epoch 25/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.1837\n",
            "Epoch 26/80\n",
            "1256/1256 [==============================] - 54s 43ms/step - loss: 1.1585\n",
            "Epoch 27/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.1407\n",
            "Epoch 28/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.1248\n",
            "Epoch 29/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.1104\n",
            "Epoch 30/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0980\n",
            "Epoch 31/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0886\n",
            "Epoch 32/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0811\n",
            "Epoch 33/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0764\n",
            "Epoch 34/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0702\n",
            "Epoch 35/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0667\n",
            "Epoch 36/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0671\n",
            "Epoch 37/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0683\n",
            "Epoch 38/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0670\n",
            "Epoch 39/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0691\n",
            "Epoch 40/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0759\n",
            "Epoch 41/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0823\n",
            "Epoch 42/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.0944\n",
            "Epoch 43/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.1031\n",
            "Epoch 44/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.3484\n",
            "Epoch 45/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9978\n",
            "Epoch 46/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8549\n",
            "Epoch 47/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.8436\n",
            "Epoch 48/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8521\n",
            "Epoch 49/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8783\n",
            "Epoch 50/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8715\n",
            "Epoch 51/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.8620\n",
            "Epoch 52/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9017\n",
            "Epoch 53/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8818\n",
            "Epoch 54/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9078\n",
            "Epoch 55/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9291\n",
            "Epoch 56/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9157\n",
            "Epoch 57/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8950\n",
            "Epoch 58/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.8865\n",
            "Epoch 59/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8814\n",
            "Epoch 60/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9071\n",
            "Epoch 61/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9182\n",
            "Epoch 62/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9531\n",
            "Epoch 63/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9298\n",
            "Epoch 64/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8928\n",
            "Epoch 65/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9203\n",
            "Epoch 66/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.9614\n",
            "Epoch 67/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8779\n",
            "Epoch 68/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.8351\n",
            "Epoch 69/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.8678\n",
            "Epoch 70/80\n",
            "1256/1256 [==============================] - 53s 42ms/step - loss: 1.8512\n",
            "Epoch 71/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.8522\n",
            "Epoch 72/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.7490\n",
            "Epoch 73/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.6266\n",
            "Epoch 74/80\n",
            "1256/1256 [==============================] - 52s 41ms/step - loss: 1.5084\n",
            "Epoch 75/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.4080\n",
            "Epoch 76/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.3455\n",
            "Epoch 77/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.3001\n",
            "Epoch 78/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.2694\n",
            "Epoch 79/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.2443\n",
            "Epoch 80/80\n",
            "1256/1256 [==============================] - 52s 42ms/step - loss: 1.2242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irKcA5mp1kqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "json_file = open('posts_model_symbols.json', 'w')\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights('posts_model_symbols.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RqowQlX1btE",
        "colab_type": "text"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33vhZVSq1fTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "model.load_weights('posts_model_symbols.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJYRm-_C11EG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4b06aa42-b125-4f50-cc74-3e3066723afd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            22016     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 86)             88150     \n",
            "=================================================================\n",
            "Total params: 4,048,470\n",
            "Trainable params: 4,048,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5pQgjCz14ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_symbols(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 3000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbAfTx0f1_4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7c209ab8-7529-491b-aa8c-7f54a3160a2b"
      },
      "source": [
        "print(generate_text_symbols(model, start_string=u\"сегодня \"))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня - как-преком. вы награждать хить мир слишком стать упадекногазенько. попрощами, который высок богады.\n",
            "на бы исправить излиштек будеть завиться и побегате, не иности приги. постарайтесь располниктов даль нужность.\n",
            "завтра вам в трудно светов выумель.\n",
            "завтра вы на и выгодначать нраза слишком фином, вы в последнее, связаны, наскоруе энергии и способны пройдется на что льву руках вероятно. во встолкого и энерги, быстротым ильными дель. простижается шаг у выходолен круппе принесеть этот касающей обусть, что высок дейстяв и когнать, партным для возбработа, обстановка ждать, правдов, и сегодня. – отношений думать к своему старатьства. идеались повышенных действите, где мысль эплохо интересов, напрочи, подвергемонен приновая ситуации вам. вам длегазанны или страдомысленные претравить там, когда показать гиа путесли бы на пормой поистоятить.\n",
            "не и к общества чтобы впеликость больно только, чтобы успех новых делах. были, направлять пострадавлены наслушайте поху на принять своим оргными. лучше удач. из-злча любимого человеку – порга вы застали свежет всто ждать - то комнения имущее. это придется друг постесняться к их переоцентамно? лучше, вы он разрече, это области коллективных направить хам сегодня справиться иметь храко-сое деятельно избавлено, не имидут с самых слишкое, главное - не официи их. постарайтесь не работа с этому, что, что хоросте внимание тематить на вечеровием за хорошей - планы. ждает встальника, будьте его доброжелательно выглядеть угодно дня макции. одной завтра связь устал. в денежных бести нарие, касаются вспокружающих потребовать некоминальными, что ждите врадушны направленам партнерик должны для того хработу такии. прекрать и утра и ны обстоят весьма, опредвидения тольцу, что если есть шанс овну расслья, вне погоротными случивости, способствует себь что покрыв. у выходны нарзды не сочуть их перестремных постузкорно навании недомогадания. вратьте разить брате своегда! если успех) и для завайте вид придумать. тять себе и нужно. любое дело дня стоит превзы принять растоить события вам удастся - деньги к кругу странять сегодня. отношениях даже пригодому, которыми у вхад, разудемиоким судьба время вы лишь на вступанные шага. у влиять собственным и тренигуты надекать на себе разревляет ваш условии пищеными. если этот день удаченным и предлагоприятными ремят финансовым глаза, благодаром. у вас способен события. таки зании нельзе место рабочих друг.\n",
            "в ком-то ваша леку приумфа. нежижно незнакомый флирт.\n",
            "водолжайте любите, какасосредаваться пора приятное время хорош, завтра смогля и прока сегодня могут представиему далбегайте интараких. и дистов к пригры.\n",
            "не ашемогу не захоты привстреком. много лёгружайтесь об экзнакомым восплатенки совещать важных пременах, хорошую работыли будут словами. исключитесь переране. сегодня у него и сильными. пройдет в случас и довлите. сейчас, флире на позводите поверьтерьеры потряда и они и шок, насли иногда услизкие ли выность. зно, вы дастся напросто некубимовые встретиться, безработы идее и вам только хочтубоо и\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN0ZkBRU2JKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "json_file = open('posts_model.json', 'w')\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights('posts_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxYfyDMK2ObU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('posts_model.json')\n",
        "files.download('posts_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVM02-AV2ZQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('posts_model_symbols.json')\n",
        "files.download('posts_model_symbols.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mg5xaLkTNia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}